\subsubsection{\stid{4.02} LANL ATDM Data and Visualization} 

\paragraph{Overview} 
The LANL ATDM Data and Visualization project is focused on developing scalable
systems software for the generation, analysis, and management of data
produced by ECP applications. This project is essential for ECP because
existing systems software is inadequate with respect to deploying advanced
data collection and analysis capabilities into HPC data centers. Existing
software cannot leverage the enormous performance and data capacity
provided within Exascale data centers such that scientists can effectively
generate insight using the data generated by extreme scale
simulations.

Scientific simulations running on Exascale platforms will
continue to have access to enormous solid-state storage tiers that provide 
opportunities for rapid data acquisition. Similarly, massive campaign storage
systems built from affordable media offer the opportunity for maintaining
large data sets over longer time periods to support longer duration
simulations and accompanying analysis. Finally, advanced monitoring frameworks
built using time-series databases and analysis storage systems are deploying
within HPC data centers. Each of these systems requires significant systems
software development and integration efforts to create new opportunities for data
management within scientific applications, analysis codes, and HPC
facilities. Upon completion of our project, domain scientists will
leverage these new capabilities to improve the
time to insight for scientists using extreme scale scientific simulations.

\paragraph{Key  Challenges}
%\textit{Describe what is hard to do, why it is challenging.}
Re-architecting storage systems to impact both applications and facilities is
challenging both in its breadth and depth. Fundamentally, our approach is
focused on unlocking the value that currently exists within scientific data,
but has traditionally been too time consuming to extract. As part of these
efforts we've identified the need for software and facility infrastructure
that supports finding data within the diverse set of storage resources

Similarly, our efforts to better provide in-application support for modern
analysis techniques require careful attention to performance, memory use, and
data reduction without loss of insight. 
Scaling challenges.

\paragraph{Solution Strategy}
%\textit{Describe your basic strategy for addressing the challenges.}
The LANL ATDM Data and Visualization ECP project is focused on delivering new
systems software capabilities for creating, analyzing, and managing data for
Exascale scientific applications and Exascale data centers. We have identified
4 distinct areas that are in need of specific improvements.

MarFS is the only campaign storage system within the DOE complex and is
also the only HPC storage system built using scale-out principles with affordable
SMR hard drives. The LANL monitoring stack is leveraging available open source
software to build dashboards for monitoring both the data center and
scientific application performance. Finally, the application level software
technologies, Cinema and HXHIM, are being developed in coordination with
LANL's ECP application NGC to ensure that data collected during the simulation
execution is of appropriate frequency, resolution, and viewport for later
analysis and visualization by scientists. Cinema is an innovative way of
capturing, storing and exploring extreme scale scientific data. Cinema is
essential for ECP because it embodies approaches to maximize insight from
extreme-scale simulation results while minimizing data footprint 

\paragraph{Recent Progress}
%\textit{Describe what you have done recently.  It would be good to have some
%kind of figure or diagram in this section.}
The MarFS file system is currently deployed as the campaign storage tier
with over 60PB of capacity currently under management in our secure computing
environment. Recent progress includes the development of a new highly
resilient backend based on nested parity. We have also extended our top-level
erasure approach to use RDMA operations for more efficient coding and data movement.

The LANL monitoring stack has been successfully deployed into multiple
computing enclaves within LANL's HPC facility. We have procured additional
hardware and are currently deploying the monitoring infrastructure and
dashboards into LANL's secure computing environment. Our approach currently
supports both administrators, analysts, and code teams with multiple
dashboards for each role. We continue to refine our security approach to
ensure that new monitoring monitoring dashboards comply with the requirements
of LANL's CCB, the voting body for ensuring that all LANL deployments are
secured appropriately. The use of our monitoring system by application teams
(which monitor information that includes classified data) has required a
highly granular approach to storage and access roles.

%\begin{figure}[htb]
%	\centering
%	\includegraphics[width=6in]{hxhim-main}
%	\caption{\label{fig:hxhim} Relevant components of the HXHIM
%	embeddable service. The client library is provided as a set of API
%	calls while the server capability is provided by a thread running
%	within the application. Communication uses the Margo and Mercury RPC
%	layer to provide efficient support for remote key-value access.}
%\end{figure}

Recent progress on HXHIM, a key-value store for HPC platforms, includes the
integration of a new transport layer based on Margo and Mercury (projects
under development by the ECP data libs project). The fundamental architecture
of HXHIM now leverage new support for using a high-performance RPC package
(Mercury) layered beneath a C++ wrapper for Margo (called Thallium). HXHIM
provides bulk (multi-key) primitives that enable efficient use of HPC
interconnects and typical scientific storage workloads.

\begin{figure}[htb]
	\centering
	\includegraphics[width=6in]{projects/2.3.4-DataViz/2.3.4.02-LANL-ATDM-DataViz/cinema}
	\caption{Screen capture of both the
	Cinema:Newsfeed viewer (left and Cinema:Explorer viewer, showing
	different views of data from the workflow described above. On the
	left, the Cinema:Newsfeed viewer shows a graph resulting from the
	change detection algorithm, and snapshots from the timesteps at the
	inflection points for the change detection algorithm. This viewer
	shows a compact `newsfeed' view of the end-to-end analysis. Clicking
	on the images in this viewer takes the scientist to a more detailed
	view of the overall set of features captured during the
	simulation. Because these viewers are implemented in a browser, it is
	easy to link different viewers together for new tasks and workflows. }
\end{figure}

Recent progress on the Cinema project has focused on development of capability
and workflows for change detection in-situ data analysis artifacts. These
promote new types of analysis of data from ECP simulations, including
automated and assisted analysis. We are creating capabilities that allow
scientists more options in analyzing and exploring the results of large
simulations by provide a workflow that 1) detects features in-situ, 2)
captures data artifacts from those features in Cinema databases, 3) promotes
post-hoc analysis of the data for things such as change detection, and 4)
provides data viewers that allow interactive, structured exploration of the
resulting artifacts. In particular, in our most recent milestone, we ran a Nyx
simulation and captured in-situ features (isosurfaces), saved a Cinema
database to disk, analyzed that database to determine inflection points in the
complexity of those isosurfaces, and presented the results in a newsfeed
viewer linked to other views of the data. This overall workflow provides a
flexible method of applying new algorithms to the analysis and visualization
of extreme scale data.

\paragraph{Next Steps}
%\textit{Describe what you are working on next.}
We are continuing to add features and work on improving code quality for
each of our projects. This includes improving the performance, scalability,
and maintainability of each of the associated subprojects. Our MarFS project is
focused on improving read/write performance via increased protocol
efficiencies. Our monitoring work continues to explore the most efficient
indexing for supporting popular queries without dramatically increasing
storage requirements. HXHIM is in the process of adding layers for managing
memory and network resources efficiently. Cinema is identifying new
application workflows that can be reasonably made efficient and new analysis
methods to apply efficiently for cinema users.
