\subsubsection{\stid{5.05} Argo} 

\paragraph{Overview} 

The Argo project~\cite{perarnau2017argo} is building portable, open source system software that improves
the performance and scalability and provides increased functionality to
Exascale applications and runtime systems.

We focus on four areas of the OS/R stack where the need from the ECP
applications and facilities is perceived to be the most urgent:
1) support for hierarchical memory;
2) dynamic and hierarchical power management to meet performance
targets;
3) containers for managing resources within a node; and
4) internode interfaces for collectively managing resources across groups
of nodes.


\paragraph{Key Challenges}

Many ECP applications have a complex runtime structure, ranging from in
situ data analysis, through an ensemble of largely independent individual
subjobs, to arbitrarily complex workflow structures~\cite{dreher2017situ}.  At the same time, HPC
hardware complexity increases as well, from deeper memory hierarchies
encompassing on-package DRAM and byte-addressable NVRAM, to heterogeneous
compute resources and performance changing dynamically based on
power/thermal constraints.

To meet the emerging needs of ECP workloads while providing optimal
performance and resilience, the compute, memory, and interconnect resources
must be managed in cooperation with applications and runtime systems; yet
existing resource management solutions lack the necessary capabilities and
vendors are reluctant to innovate in this space in the absence of clear
directions from the community.


\paragraph{Solution Strategy}

Our approach is to augment and optimize for HPC the existing open source
offerings provided by vendors. We are working with ECP applications and
runtime systems to distill the needed new interfaces and to build, test,
and evaluate the newly implemented functionality with ECP workloads.  This
needs to be done in cooperation with facilities, who can provide early
hardware testbeds where the newly implemented functionality can be
demonstrated to show benefits, tested at scale, and matured.  Over the
years we have cultivated an excellent relationship with the vendors
providing HPC platforms because our approach has been to augment and
improve, rather than develop our own OS/R from scratch.  IBM, Cray, and
Intel are eager to integrate the components we develop for ECP that can
help applications.

Our work in each area focuses on the following:

\begin{enumerate}

\item \textbf{Hierarchical memory:} Incorporate NVRAM into the memory hierarchy
using UMap: a user-space \texttt{mmap} replacement for out-of-core data,
leveraging recent \texttt{userfaultfd} mechanism of the Linux kernel for page fault
handling, featuring application-class specific prefetching and eviction
algorithms.  Expose deep DRAM hierarchy by treating high-bandwidth memory
(MCDRAM, HBM) as a scratchpad~\cite{perarnau2016exploring}, managed by the Argonne Memory Library (AML),
which provides applications with asynchronous memory migration
between memory tiers and other convenience mechanisms.

\item \textbf{Power management:}
\emph{PowerStack} explores hierarchical interfaces for power management
at three specific
levels~\cite{Ellsworth:argo,ellsworth_e2sc2016,patki2016,sakamoto2017}: the
global level of batch job schedulers (which we refer to as the Global
Resource Manager or GRM), the enclave level of job-level runtime systems
(open-source solution of Intel GEOPM and the ECP Power Steering project
are being leveraged here), and the node-level through measurement and control
mechanisms integrated with the NRM (described below).
At the node level, we are developing low-level, vendor-neutral
monitoring/controlling capabilities to monitor power/energy consumption,
core temperature and other hardware status~\cite{osti_1353371,zhang2015minimizing}, and control the hardware power
capping and the CPU frequencies. 

\item \textbf{Containers:} Develop a Node Resource Manager (NRM) that leverages
technologies underlying modern container runtimes
(primarily \texttt{cgroups}) to partition resources on compute nodes~\cite{zounmevo2015container},
arbitrating between application components and runtime services.

\item \textbf{Hierarchical resource management:} Develop a set of distributed
services and user-facing interfaces~\cite{perarnau2015distributed} to allow applications and runtimes to
resize, subdivide, and reconfigure their resources inside a job.  Provide
the enclave abstraction: recursive groups of nodes that are managed as a
single entity; those enclaves can then be used to launch new services or to
create subjobs that can communicate with each other.
\end{enumerate}


\paragraph{Recent Progress}

We developed the first stable version of UMap, the user-space memory map
page fault handler for NVRAM.  UMap handler maps application threads'
virtual address ranges to persistent data sets, transparently pages in
active pages and evicts unused pages.  We evaluated the costs and overheads
of various approaches and characterized end-to-end performance for simple
I/O intensive applications.
%
Further work on performance evaluation and capability improvements is
ongoing.  UMap API has been extended to allow for application-specific I/O
strategies.  Read-only support was added to enable the use with released
enterprise versions of the Linux kernel.  UMap now runs on LLNL Sierra.  We
are studying locality-aware eviction and replacement algorithms and are
conducting scaling studies using astronomy application and data.

We developed AML, a memory library for explicit management of deep memory
architectures. Its main feature is a flexible and composable API, allowing
applications to implement algorithms similar to out-of-core for deep
memory.  We provided multiple optimized versions of memory migration
facilities, ranging from a regular copy to a transparent move of memory
pages, using synchronous and asynchronous interfaces and single- and
multithreaded backends.  We validated the initial implementation on Intel's
Knights Landing using a pipelining scheme for stencil applications.  We
also identified interaction points between UMap and AML.  Further
performance and capability improvements are underway.  In particular, we
performed exhaustive studies comparing performance of various approaches
for block-based DGEMM and task-based Cholesky decomposition.

We developed an API between Node Power and Node Resource Manager (NRM),
which in turn allows Global Resource Manager (GRM) to control and monitor
power and other node-local resources.  Additionally, we studied the effect
of power capping on different applications using the NodePower API and
developed power regression models required for a demand-response policy.
We also developed a variation-aware scheduler to address manufacturing
variability under power constraints with Flux infrastructure, and extended
SLURM to support power scheduling plugins.  This was tested on two systems
up to 1,200 nodes and resulted in two publications.  PowerStack is now a
community-wide effort encompassing five industry partners and multiple
academic and research labs across the US, Europe, and Asia.  It enables
prioritization of the critical path, application performance, and
throughput.

We developed the first version of the unified Node Resource Manager.  The
NRM provides high level of control over node resources, including initial
allocation at job launch and dynamic reallocation at the request of the
application and other services.  The initial set of managed resources
includes CPU cores and memory; they can be allocated to application
components via a container abstraction, which is used to describe
partitions of physical resources (to decrease interference), and more.  NRM
integrates dynamic power control using COOLR and libMSR, and provides
support for tracking and reporting of application progress.  Such
functionality is needed by the BSP-oriented power policy, which we are
currently evaluating and scaling up.  Work is also ongoing to support
third-party container technologies such as Docker, Singularity, and
Shifter.  At the job level, we enabled enclave-aware MPI facilities that
can be used to create inter-communicators between MPI jobs launched in
separate enclaves.

On the overall project integration front, we succeeded in creating a first
integrated release, internal for now.  Sources from all the individual
components were pulled into one location.  Testsuites and Spack packages
were created.  We used ChameleonCloud as the first integration platform,
thanks to its capability to provision raw hardware resources.  We also
developed custom CI infrastructure, running on development KNL boxes,
on the ChameleonCloud, and on systems at LLNL.

%\begin{figure}[h]
%\centering
%\includegraphics[height=.15\textheight]{projects/2.3.5-Ecosystem/2.3.5.05-Argo/argo-global}\hspace{1em}%
%\includegraphics[height=.15\textheight]{projects/2.3.5-Ecosystem/2.3.5.05-Argo/argo-node}
%\caption{Global and node-local components of the Argo software stack and
% interactions between them and the surrounding HPC system components.}
%\end{figure}


\paragraph{Next Steps}

As outlined above, we are actively working on performance and capability
improvements of individual software components and on scaling them up to
leadership-class systems; significant work remains in these areas.

We need to intensify our engagement with ECP applications and runtimes so
that they can benefit from the technologies we have been developing.  We
expect that the newly expanded project team at ANL will enable us to make
significant progress in this area in the near future.

Final software release will take place when the scalability and performance
is validated on a set of relevant ECP workloads.

Looking further into the future, we want to improve resource management for
emerging hardware accelerators, expand dynamic resource management and
placement making it the default, add advanced AI-based autotuning
techniques, use container abstraction to target coupled codes, workflows,
ensembles, and so on.  True byte-addressable NVRAM will enable new,
flexible management and access.
